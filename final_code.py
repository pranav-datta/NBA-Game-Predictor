# -*- coding: utf-8 -*-
"""Final Code

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1y0XEfxxXnflzQoPgBQjjXdqEZg8m18gf
"""

# Commented out IPython magic to ensure Python compatibility.
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
# %matplotlib inline

from google.colab import drive

drive.mount('/content/gdrive/', force_remount=True)
# df_decades = pd.read_csv("/content/gdrive/MyDrive/CS 7641/")

# Commented out IPython magic to ensure Python compatibility.
# %cd

#read the 20s dataset
df_20s = pd.read_csv("/content/gdrive/MyDrive/CS 7641/NBA Dataset Decades - 20s.csv")
df_20s.head()

#read the 10s dataset
df_10s = pd.read_csv("/content/gdrive/MyDrive/CS 7641/NBA Dataset Decades - 10s.csv")
df_10s.head()

#read the 00s dataset
df_00s = pd.read_csv("/content/gdrive/MyDrive/CS 7641/NBA Dataset Decades - 00s.csv")
df_00s.head()

#read the 90s dataset
df_90s = pd.read_csv("/content/gdrive/MyDrive/CS 7641/NBA Dataset Decades - 90s.csv")
df_90s.head()

#read the 80s dataset
df_80s = pd.read_csv("/content/gdrive/MyDrive/CS 7641/NBA Dataset Decades - 80s.csv")
df_80s.head()

#col names for 20s
df_pca_20s = df_20s.loc[:, ['fgm', 'fga', 'fg3m', 'fg3a', 'ftm', 'fta', 'reb', 'ast', 'stl', 'blk', 'tov', 'pts']]

col_name_20s = df_pca_20s.columns.tolist()

#col names for 10s
df_pca_10s = df_10s.loc[:, ['fgm', 'fga', 'fg3m', 'fg3a', 'ftm', 'fta', 'reb', 'ast', 'stl', 'blk', 'tov', 'pts']]

col_name_10s = df_pca_10s.columns.tolist()

#col names for 00s
df_pca_00s = df_00s.loc[:, ['fgm', 'fga', 'fg3m', 'fg3a', 'ftm', 'fta', 'reb', 'ast', 'stl', 'blk', 'tov', 'pts']]

col_name_00s = df_pca_00s.columns.tolist()

#col names for 90s
df_pca_90s = df_90s.loc[:, ['fgm', 'fga', 'fg3m', 'fg3a', 'ftm', 'fta', 'reb', 'ast', 'stl', 'blk', 'tov', 'pts']]

col_name_90s = df_pca_90s.columns.tolist()

#col names for 10s
df_pca_80s = df_80s.loc[:, ['fgm', 'fga', 'fg3m', 'fg3a', 'ftm', 'fta', 'reb', 'ast', 'stl', 'blk', 'tov', 'pts']]

col_name_80s = df_pca_80s.columns.tolist()

df_pca_20s.corr()

df_pca_10s.corr()

df_pca_00s.corr()

df_pca_90s.corr()

df_pca_80s.corr()

X_20s = df_pca_20s.iloc[:,1:81821].values
Y = df_pca_20s.iloc[:,0].values

x_20 = StandardScaler().fit_transform(X_20s)
x_20

X_10s = df_pca_10s.iloc[:,1:81821].values
Y = df_pca_10s.iloc[:,0].values

x_10 = StandardScaler().fit_transform(X_10s)
x_10

X_00s = df_pca_00s.iloc[:,1:81821].values
Y = df_pca_00s.iloc[:,0].values

x_00 = StandardScaler().fit_transform(X_00s)
x_00

X_90s = df_pca_90s.iloc[:,1:81821].values
Y = df_pca_90s.iloc[:,0].values

x_90 = StandardScaler().fit_transform(X_90s)
x_90

X_80s = df_pca_80s.iloc[:,1:81821].values
Y = df_pca_80s.iloc[:,0].values

x_80 = StandardScaler().fit_transform(X_80s)
x_80

#get rid of inf and NANs from datasets
x_20s_new = x_20[np.isfinite(x_20).all(1)]
x_10s_new = x_10[np.isfinite(x_10).all(1)]
x_00s_new = x_00[np.isfinite(x_00).all(1)]
x_90s_new = x_90[np.isfinite(x_90).all(1)]
x_80s_new = x_80[np.isfinite(x_80).all(1)]

#covariance matrix for 20s
mean_vec_20s = np.mean(x_20s_new, axis=0)
cov_mat_20s = (x_20s_new - mean_vec_20s).T.dot((x_20s_new - mean_vec_20s)) / (x_20s_new.shape[0]-1)
print('Covariance matrix \n%s' %cov_mat_20s)

#covariance matrix for 10s
mean_vec_10s = np.mean(x_10s_new, axis=0)
cov_mat_10s = (x_10s_new - mean_vec_10s).T.dot((x_10s_new - mean_vec_10s)) / (x_10s_new.shape[0]-1)
print('Covariance matrix \n%s' %cov_mat_10s)

#covariance matrix for 00s
mean_vec_00s = np.mean(x_00s_new, axis=0)
cov_mat_00s = (x_00s_new - mean_vec_00s).T.dot((x_00s_new - mean_vec_00s)) / (x_00s_new.shape[0]-1)
print('Covariance matrix \n%s' %cov_mat_00s)

#covariance matrix for 90s
mean_vec_90s = np.mean(x_90s_new, axis=0)
cov_mat_90s = (x_90s_new - mean_vec_90s).T.dot((x_90s_new - mean_vec_90s)) / (x_90s_new.shape[0]-1)
print('Covariance matrix \n%s' %cov_mat_90s)

#covariance matrix for 80s
mean_vec_80s = np.mean(x_80s_new, axis=0)
cov_mat_80s = (x_80s_new - mean_vec_80s).T.dot((x_80s_new - mean_vec_80s)) / (x_80s_new.shape[0]-1)
print('Covariance matrix \n%s' %cov_mat_80s)

eig_vals_20s, eig_vecs_20s = np.linalg.eig(cov_mat_20s)

for i in eig_vecs_20s:
    print(i)

eig_vals_10s, eig_vecs_10s = np.linalg.eig(cov_mat_10s)

for i in eig_vecs_10s:
    print(i)

eig_vals_00s, eig_vecs_00s = np.linalg.eig(cov_mat_00s)

for i in eig_vecs_00s:
    print(i)

eig_vals_90s, eig_vecs_90s = np.linalg.eig(cov_mat_90s)

for i in eig_vecs_90s:
    print(i)

eig_vals_80s, eig_vecs_80s = np.linalg.eig(cov_mat_80s)

for i in eig_vecs_80s:
    print(i)

eig_pairs_20s = [(np.abs(eig_vals_20s[i]), eig_vecs_20s[:,i]) for i in range(len(eig_vals_20s))]

# Sort the (eigenvalue, eigenvector) tuples from high to low
eig_pairs_20s.sort(key=lambda x: x[0], reverse=True)

# Visually confirm that the list is correctly sorted by decreasing eigenvalues
print('Eigenvalues in descending order:')
num = 0
for i in eig_pairs_20s:
    print(i[0])
    num += 1
print(num)

eig_pairs_10s = [(np.abs(eig_vals_10s[i]), eig_vecs_10s[:,i]) for i in range(len(eig_vals_10s))]

# Sort the (eigenvalue, eigenvector) tuples from high to low
eig_pairs_10s.sort(key=lambda x: x[0], reverse=True)

# Visually confirm that the list is correctly sorted by decreasing eigenvalues
print('Eigenvalues in descending order:')
num = 0
for i in eig_pairs_10s:
    print(i[0])
    num += 1
print(num)

eig_pairs_00s = [(np.abs(eig_vals_00s[i]), eig_vecs_00s[:,i]) for i in range(len(eig_vals_00s))]

# Sort the (eigenvalue, eigenvector) tuples from high to low
eig_pairs_00s.sort(key=lambda x: x[0], reverse=True)

# Visually confirm that the list is correctly sorted by decreasing eigenvalues
print('Eigenvalues in descending order:')
num = 0
for i in eig_pairs_00s:
    print(i[0])
    num += 1
print(num)

eig_pairs_90s = [(np.abs(eig_vals_90s[i]), eig_vecs_90s[:,i]) for i in range(len(eig_vals_90s))]

# Sort the (eigenvalue, eigenvector) tuples from high to low
eig_pairs_90s.sort(key=lambda x: x[0], reverse=True)

# Visually confirm that the list is correctly sorted by decreasing eigenvalues
print('Eigenvalues in descending order:')
num = 0
for i in eig_pairs_90s:
    print(i[0])
    num += 1
print(num)

eig_pairs_80s = [(np.abs(eig_vals_80s[i]), eig_vecs_80s[:,i]) for i in range(len(eig_vals_80s))]

# Sort the (eigenvalue, eigenvector) tuples from high to low
eig_pairs_80s.sort(key=lambda x: x[0], reverse=True)

# Visually confirm that the list is correctly sorted by decreasing eigenvalues
print('Eigenvalues in descending order:')
num = 0
for i in eig_pairs_80s:
    print(i[0])
    num += 1
print(num)

tot = sum(eig_vals_20s)
var_exp = [(i / tot)*100 for i in sorted(eig_vals_20s, reverse=True)]
with plt.style.context('dark_background'):
    plt.figure(figsize=(6, 4))

    plt.bar(range(11), var_exp, alpha=0.5, align='center',
            label='individual explained variance for 2020s')
    plt.ylabel('Explained variance ratio')
    plt.xlabel('Principal components')
    plt.legend(loc='best')
    plt.tight_layout()

tot = sum(eig_vals_10s)
var_exp = [(i / tot)*100 for i in sorted(eig_vals_10s, reverse=True)]
with plt.style.context('dark_background'):
    plt.figure(figsize=(6, 4))

    plt.bar(range(11), var_exp, alpha=0.5, align='center',
            label='individual explained variance for 2010s')
    plt.ylabel('Explained variance ratio')
    plt.xlabel('Principal components')
    plt.legend(loc='best')
    plt.tight_layout()

tot = sum(eig_vals_00s)
var_exp = [(i / tot)*100 for i in sorted(eig_vals_00s, reverse=True)]
with plt.style.context('dark_background'):
    plt.figure(figsize=(6, 4))

    plt.bar(range(11), var_exp, alpha=0.5, align='center',
            label='individual explained variance for 2000s')
    plt.ylabel('Explained variance ratio')
    plt.xlabel('Principal components')
    plt.legend(loc='best')
    plt.tight_layout()

tot = sum(eig_vals_90s)
var_exp = [(i / tot)*100 for i in sorted(eig_vals_90s, reverse=True)]
with plt.style.context('dark_background'):
    plt.figure(figsize=(6, 4))

    plt.bar(range(11), var_exp, alpha=0.5, align='center',
            label='individual explained variance for 1990s')
    plt.ylabel('Explained variance ratio')
    plt.xlabel('Principal components')
    plt.legend(loc='best')
    plt.tight_layout()

tot = sum(eig_vals_80s)
var_exp = [(i / tot)*100 for i in sorted(eig_vals_80s, reverse=True)]
with plt.style.context('dark_background'):
    plt.figure(figsize=(6, 4))

    plt.bar(range(11), var_exp, alpha=0.5, align='center',
            label='individual explained variance for 1980s')
    plt.ylabel('Explained variance ratio')
    plt.xlabel('Principal components')
    plt.legend(loc='best')
    plt.tight_layout()

pca = PCA().fit(x_20s_new)
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlim(0,11,1)
plt.title("2020s")
plt.xlabel('Number of components')
plt.ylabel('Cumulative explained variance')

pca = PCA().fit(x_10s_new)
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlim(0,11,1)
plt.title("2010s")
plt.xlabel('Number of components')
plt.ylabel('Cumulative explained variance')

pca = PCA().fit(x_00s_new)
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlim(0,11,1)
plt.title("2000s")
plt.xlabel('Number of components')
plt.ylabel('Cumulative explained variance')

pca = PCA().fit(x_90s_new)
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlim(0,11,1)
plt.title("1990s")
plt.xlabel('Number of components')
plt.ylabel('Cumulative explained variance')

pca = PCA().fit(x_80s_new)
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlim(0,11,1)
plt.title("1980s")
plt.xlabel('Number of components')
plt.ylabel('Cumulative explained variance')

x_reduced = x_new.dot(eig_vecs[:,:10])
x_reduced

x_reduced.shape

# Fit Linear Regression Model
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

features = x_reduced #x
labels = df.loc[:, ['win']].values

# print(labels)

# graphs
print(labels)
win_features = x_reduced[np.where(labels == 1), :]
print(win_features[0,:])
print(x_reduced.shape)

# Split data into train and test
xtrain, xtest, ytrain, ytest = train_test_split(features, labels, test_size=0.2, random_state=42)
print(xtrain.shape)
print(ytrain)

lin_reg = LinearRegression().fit(xtrain, ytrain)

from sklearn.metrics import classification_report, mean_squared_error, r2_score

ypreds_test = lin_reg.predict(xtest)

print("Testing:")
print("MSE:", mean_squared_error(ytest, ypreds_test))
print("R2:", r2_score(ytest, ypreds_test))

print("")
# ypreds_train = lin_reg.predict(xtrain)

# classification_report(ytest, ypreds_test)


ypreds_train = lin_reg.predict(xtrain)

print("Training:")
print("MSE:", mean_squared_error(ytrain, ypreds_train))
print("R2:", r2_score(ytrain, ypreds_train))

print("preds")
print(ypreds_train)
print("actual")
print(ytrain)

from sklearn.metrics import classification_report, mean_squared_error, r2_score
from sklearn.linear_model import LogisticRegression

log_reg = LogisticRegression().fit(xtrain, ytrain)


ypreds_test = log_reg.predict(xtest)

print("Testing:")
print("MSE:", mean_squared_error(ytest, ypreds_test))
print("R2:", r2_score(ytest, ypreds_test))

print("")
# ypreds_train = lin_reg.predict(xtrain)




ypreds_train = log_reg.predict(xtrain).reshape(-1,1)

print("Training:")
print("MSE:", mean_squared_error(ytrain, ypreds_train))
print("R2:", r2_score(ytrain, ypreds_train))


print("")
print("preds")
print(ypreds_train)
print("actual")
print(ytrain)

from sklearn.metrics import f1_score, precision_score

print(classification_report(ytest, ypreds_test))
# f1_score(ytest, ypreds_test)

print(classification_report(ytrain, ypreds_train))

from sklearn import svm

clf = svm.SVC(kernel='poly', degree=4)
clf.fit(xtrain, ytrain)

train_preds = clf.predict(xtest)

print(classification_report(ytest, train_preds))

from sklearn.ensemble import RandomForestClassifier

xtrain, xtest, ytrain, ytest = train_test_split(features, labels, test_size=0.2, random_state=42)

# instantiate model
model = RandomForestClassifier(n_estimators=200, random_state=0)
# fit model
model.fit(xtrain, ytrain)

importances = model.feature_importances_

# visualization
cols = col_names
(pd.DataFrame(importances, cols, columns = ['importance'])
 .sort_values(by='importance', ascending=True)
 .plot(kind='barh', figsize=(4,10)))

